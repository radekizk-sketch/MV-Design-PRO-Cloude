"""
Batch Execution Service — PR-20: Deterministic Batch Orchestration

Application service for executing multiple FaultScenarios in a single,
deterministic, sequential batch.

INVARIANTS:
- SEQUENTIAL execution only (v1)
- ZERO parallelism
- ZERO retry
- ZERO partial success — any failure → FAILED
- readiness false → FAILED
- eligibility false → FAILED
- solver exception → FAILED
- Does NOT modify solvers, ResultSet API, or ExecutionEngine contract
"""

from __future__ import annotations

import logging
from typing import Any
from uuid import UUID

from domain.batch_job import (
    BatchJob,
    BatchJobStatus,
    new_batch_job,
)
from domain.execution import (
    ExecutionAnalysisType,
    RunStatus,
)
from application.execution_engine.service import ExecutionEngineService
from application.execution_engine.errors import (
    ExecutionError,
    RunNotFoundError,
    StudyCaseNotFoundError,
)

logger = logging.getLogger(__name__)


class BatchExecutionError(Exception):
    """Base exception for batch execution errors."""

    pass


class BatchNotFoundError(BatchExecutionError):
    """BatchJob does not exist."""

    def __init__(self, batch_id: str) -> None:
        super().__init__(f"Batch nie istnieje: {batch_id}")
        self.batch_id = batch_id


class BatchNotPendingError(BatchExecutionError):
    """BatchJob is not in PENDING status."""

    def __init__(self, batch_id: str, status: str) -> None:
        super().__init__(
            f"Batch {batch_id} ma status {status} — wymagany PENDING"
        )
        self.batch_id = batch_id
        self.status = status


class BatchExecutionService:
    """
    Orchestration service for deterministic batch execution.

    Executes multiple scenarios sequentially through the ExecutionEngine.
    No parallelism, no retry, no partial success.
    """

    def __init__(self, engine: ExecutionEngineService) -> None:
        self._engine = engine
        self._batches: dict[UUID, BatchJob] = {}
        self._case_batches: dict[UUID, list[UUID]] = {}

    def create_batch_job(
        self,
        *,
        study_case_id: UUID,
        analysis_type: ExecutionAnalysisType,
        scenario_ids: list[UUID],
        scenario_content_hashes: list[str],
        solver_inputs: list[dict[str, Any]],
        readiness: dict[str, Any] | None = None,
        eligibility: dict[str, Any] | None = None,
    ) -> BatchJob:
        """
        Create a new BatchJob in PENDING status.

        Validates study case exists and stores the batch.
        Solver inputs are stored for sequential execution.

        Args:
            study_case_id: Study case this batch belongs to.
            analysis_type: Analysis type for all scenarios.
            scenario_ids: Scenario UUIDs (will be sorted).
            scenario_content_hashes: Content hashes per scenario.
            solver_inputs: Solver input dicts per scenario (same order as scenario_ids).
            readiness: Optional readiness check result.
            eligibility: Optional eligibility check result.

        Returns:
            Created BatchJob in PENDING status.

        Raises:
            StudyCaseNotFoundError: If study case not registered.
            ValueError: If inputs have mismatched lengths or duplicates.
        """
        # Verify study case exists
        self._engine.get_study_case(study_case_id)

        if len(scenario_ids) != len(solver_inputs):
            raise ValueError(
                "scenario_ids i solver_inputs muszą mieć tę samą długość"
            )

        batch = new_batch_job(
            study_case_id=study_case_id,
            analysis_type=analysis_type,
            scenario_ids=scenario_ids,
            scenario_content_hashes=scenario_content_hashes,
        )

        # Store solver inputs indexed by scenario_id for execution
        # Sort to match the sorted scenario_ids in the batch
        paired = sorted(
            zip(scenario_ids, solver_inputs, scenario_content_hashes),
            key=lambda p: str(p[0]),
        )
        self._solver_inputs: dict[UUID, dict[UUID, dict[str, Any]]] = getattr(
            self, "_solver_inputs", {}
        )
        self._batch_readiness: dict[UUID, dict[str, Any] | None] = getattr(
            self, "_batch_readiness", {}
        )
        self._batch_eligibility: dict[UUID, dict[str, Any] | None] = getattr(
            self, "_batch_eligibility", {}
        )

        self._solver_inputs[batch.batch_id] = {
            p[0]: p[1] for p in paired
        }
        self._batch_readiness[batch.batch_id] = readiness
        self._batch_eligibility[batch.batch_id] = eligibility

        # Store batch
        self._batches[batch.batch_id] = batch
        if study_case_id not in self._case_batches:
            self._case_batches[study_case_id] = []
        self._case_batches[study_case_id].append(batch.batch_id)

        logger.info(
            "Created batch %s for case %s with %d scenarios, hash=%s",
            batch.batch_id,
            study_case_id,
            len(batch.scenario_ids),
            batch.batch_input_hash[:16],
        )

        return batch

    def execute_batch(self, batch_id: UUID) -> BatchJob:
        """
        Execute a batch job sequentially.

        FLOW:
        1. Validate batch is PENDING
        2. Mark as RUNNING
        3. For each scenario (in sorted order):
            a. Create Run via ExecutionEngine
            b. Execute Run (start_run + complete_run)
            c. Collect run_id and result_set_id
        4. On success: mark DONE with all run_ids and result_set_ids
        5. On ANY failure: mark FAILED immediately, no retry

        Args:
            batch_id: ID of the batch to execute.

        Returns:
            Updated BatchJob (DONE or FAILED).

        Raises:
            BatchNotFoundError: If batch doesn't exist.
            BatchNotPendingError: If batch is not PENDING.
        """
        batch = self._get_batch(batch_id)
        if batch.status != BatchJobStatus.PENDING:
            raise BatchNotPendingError(str(batch_id), batch.status.value)

        # Transition to RUNNING
        batch = batch.mark_running()
        self._batches[batch_id] = batch

        solver_inputs = self._solver_inputs.get(batch_id, {})
        readiness = self._batch_readiness.get(batch_id)
        eligibility = self._batch_eligibility.get(batch_id)

        collected_run_ids: list[UUID] = []
        collected_result_set_ids: list[UUID] = []

        # SEQUENTIAL execution — no parallelism
        for scenario_id in batch.scenario_ids:
            si = solver_inputs.get(scenario_id, {})
            try:
                # Create Run
                run = self._engine.create_run(
                    study_case_id=batch.study_case_id,
                    analysis_type=batch.analysis_type,
                    solver_input=si,
                    readiness=readiness,
                    eligibility=eligibility,
                )

                # Start Run
                run = self._engine.start_run(run.id)

                # Complete Run (simplified — real execution would call solver)
                # For batch orchestration, we complete with the solver input as results
                # The actual solver execution is done by execute_run_sc in the engine
                run, result_set = self._engine.complete_run(
                    run.id,
                    validation_snapshot={"batch_id": str(batch_id)},
                    readiness_snapshot=readiness or {},
                    element_results=[],
                    global_results=si.get("expected_results", {}),
                )

                collected_run_ids.append(run.id)
                collected_result_set_ids.append(run.id)  # ResultSet keyed by run_id

                logger.info(
                    "Batch %s: scenario %s completed, run=%s",
                    batch_id,
                    scenario_id,
                    run.id,
                )

            except Exception as exc:
                error_msg = f"Scenariusz {scenario_id}: {exc}"
                logger.warning(
                    "Batch %s FAILED at scenario %s: %s",
                    batch_id,
                    scenario_id,
                    exc,
                )
                batch = batch.mark_failed(
                    errors=(error_msg,),
                    run_ids=tuple(collected_run_ids),
                    result_set_ids=tuple(collected_result_set_ids),
                )
                self._batches[batch_id] = batch
                return batch

        # All scenarios completed successfully
        batch = batch.mark_done(
            run_ids=tuple(collected_run_ids),
            result_set_ids=tuple(collected_result_set_ids),
        )
        self._batches[batch_id] = batch

        logger.info(
            "Batch %s DONE: %d runs completed",
            batch_id,
            len(collected_run_ids),
        )

        return batch

    def get_batch(self, batch_id: UUID) -> BatchJob:
        """Get a batch job by ID."""
        return self._get_batch(batch_id)

    def list_batches(self, study_case_id: UUID) -> list[BatchJob]:
        """List all batches for a study case, newest first."""
        batch_ids = self._case_batches.get(study_case_id, [])
        batches = [
            self._batches[bid]
            for bid in batch_ids
            if bid in self._batches
        ]
        return list(reversed(batches))

    def _get_batch(self, batch_id: UUID) -> BatchJob:
        """Get a batch or raise BatchNotFoundError."""
        batch = self._batches.get(batch_id)
        if batch is None:
            raise BatchNotFoundError(str(batch_id))
        return batch
